{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://tau-data.id/umi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img alt=\"\" src=\"images/0_Cover.jpg\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">Modul 06: Pendahuluan Machine Learning II</font></center>\n",
    "<b><center>(C) Taufik Sutanto - 2019</center>\n",
    "<center>tau-data Indonesia ~ https://tau-data.id ~ taufik@tau-data.id</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color=\"blue\">Pendahuluan Machine Learning II: Supervised Methods</font></center>\n",
    "<img alt=\"\" src=\"images/PDS_logo.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color=\"blue\">Workshop Schedule</font>\n",
    "\n",
    "## <font color=\"green\">Hari ke-3 (Rabu, 29 Jan 2020)</font>\n",
    "\n",
    "**Pendahuluan Data Science II**\n",
    "* 09:00 – 11:00 Unsupervised Learning & Regresi\n",
    "* 11:00 – 12:00\tLatihan\n",
    "* 13:00 – 15:00\tSupervised Learning - Klasifikasi\n",
    "* 15:00 – 16.00\tLatihan Klasifikasi\n",
    "\n",
    "Studi Kasus: **Human Resources Analytics**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Modules\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "!pip install graphviz\n",
    "!pip install dtreeviz\n",
    "import graphviz, pandas as pd, matplotlib.pyplot as plt, numpy as np, seaborn as sns\n",
    "from pandas.plotting import scatter_matrix \n",
    "from sklearn import model_selection, tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from dtreeviz.trees import *\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outline:\n",
    "\n",
    "* Review Kuliah Sebelumnya\n",
    "* Some quick (Python) codes for the previous lectures \n",
    "* Decision Tree, \n",
    "* Random Forest, \n",
    "* Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Review:\n",
    "<img alt=\"\" src=\"images/KDD.jpg\" style=\"width: 650px; height: 376px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rule: Rule, Support, Confidence, Lift by Example\n",
    "<img alt=\"\" src=\"images/Rule_Lift_Support_Confidence.png\" style=\"width: 300px ; height: 181px\" />\n",
    "<img alt=\"\" src=\"images/Rule_Lift_Support_Confidence_example.png\" style=\"width: 300px; height: 222px;\" />\n",
    "* http://www.saedsayad.com/association_rules.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "<img alt=\"\" src=\"images/reg_to_log.png\" style=\"width: 650px; height: 328px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kembali ke Teori dulu sebelum melanjutkan aplikasinya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/Sejarah_ML_AI.jpg\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sejarah Metode Konvensional di ML/DS\n",
    "\n",
    "* Bayesian\t\t: 1763 Thomas Bayes, Statisticians\n",
    "* Regresi\t\t: Abad 19 (~18XX), Francis Galton (biologist)\n",
    "* Decision Tree\t: 1959, William Belson, biologist\u000b",
    "\t\t\t  (popular oleh R. Quinlan 80-an ID3 & CART oleh )\n",
    "* Perceptron\t\t: 1962, Frank Rosenblatt, psychologist\n",
    "* SVM\t\t: 1963, Vapnik, Mathematics and Statistics.\n",
    "* Neural network\t: 1974, Werbos (backprop), 1990 Hecht-Nielsen (MLP)(setelah perceptron)\n",
    "* Semi-Supervised\t: 1965 Scruder, tapi popular 2008 Zhu (comp scientist)\n",
    "* Ensemble\t\t: 1979, Dasarathy, \n",
    "* Deep learning\t: in more detail in the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularitas DL\n",
    "\n",
    "* Ketika ML (SVM) sedang populer di tahun 2010, beberapa peneliti tetap mendalami NN, diantaranya nama-nama tenar seperti Geoffrey Hinton di the University of Toronto, dan Yann LeCun di New York University.\n",
    "* Deep Learning dengan GPU dimulai di tahun 2011 (Dan Ciresan dari  IDSIA-Swiss)\n",
    "* Namun di 2012 baru terkenal karena permasalahan klasifikasi ImageNet (~1,4 juta image dikategorikan ke 1000 kelas) mampu diselesaikan oleh Hinton. Awalnya akurasinya 'hanya' ~74% (2011), lalu ~83(2012), dan di anggap telah solved di 2015 dengan akurasi ~96% (Deep Convolutional Neural Network - convnets).\n",
    "* Sejak itu convnets menjadi model dasar computer vision.\n",
    "* Di Kaggle (sekitar 2016) ada trend untuk data terstruktur biasanya diselesaikan dengan Gradient Boosting (e.g. XGBoost) dan data tidak terstruktur dengan Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/dataScience_models.gif\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "<img alt=\"\" src=\"images/6_DT.png\" style=\"height:336px; width:904px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/6_DT_meme.png\" style=\"height:342px; width:460px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Explanation of a Decision Tree\n",
    "\n",
    "* http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n",
    "* http://www.r2d3.us/visual-intro-to-machine-learning-part-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More resources:\n",
    "\n",
    "* http://www.saedsayad.com/decision_tree.htm\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Asumsi (induktif bias) dari&nbsp; Decision tree</h3>\n",
    "<img alt=\"\" src=\"images/6_asumsi_DT.JPG\" style=\"height:300px; width:300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lebih Tepatnya\n",
    "<p><img alt=\"\" src=\"images/Dec_Tree_Asumsi_Depth.png\" style=\"width: 650px; height: 122px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teori Decision Tree : Information theory\n",
    "<img alt=\"\" src=\"images/dec_Tree_Theory.png\" style=\"width: 600px; height: 337px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/Entropy.png\" style=\"width: 650px; height: 290px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/Information_Gain.png\" style=\"width: 650px; height: 199px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/Contoh_Entropy.png\" style=\"width: 469px; height: 339px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/Contoh_Gain.png\" style=\"width: 650px; height: 456px;\" /></p>\n",
    "* Contoh Lain: http://www.saedsayad.com/decision_tree.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative to Information Gain : Gini Index (CART)\n",
    "https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Belum (tidak) Dibahas\n",
    "* Impurity Functions\n",
    "* Prunning\n",
    "* Numeric variables\n",
    "* Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><u><strong>When to use:</strong></u></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Target : Binomial/nominal.</li>\n",
    "\t<li>Predictors (input): binomial, nominal, and-or interval (ratio).</li>\n",
    "</ul>\n",
    "\n",
    "<p><u><strong>Advantage:</strong></u></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Fast and embarrassingly parallel.</li>\n",
    "\t<li>Tanpa iterasi, cocok untuk&nbsp;Big Data technology (e.g. Hadoop)[map-reduce friendly]</li>\n",
    "\t<li>Interpretability</li>\n",
    "\t<li>Robust terhadap outliers &amp; missing values</li>\n",
    "</ul>\n",
    "\n",
    "<p><u><strong>Disadvantage:</strong></u></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Non probabilistic (ad hoc heuristic) +/-</li>\n",
    "\t<li>Target dengan banyak kelas</li>\n",
    "\t<li>Sensitive (instability)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFile CSV\n",
    "try:\n",
    "    df = pd.read_csv('data/iris.csv') # run locally\n",
    "except:\n",
    "    !wget https://raw.githubusercontent.com/taufikedys/tau-data/master/data/iris.csv # \"Google Colab\"\n",
    "    df = pd.read_csv('iris.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SPECIES'] = df['SPECIES'].astype('category')\n",
    "print(df.groupby('SPECIES').size())\n",
    "df.describe(include='all') # try without the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Data\n",
    "X = df[['SEPALLEN','SEPALWID','PETALLEN','PETALWID']]\n",
    "Y = df['SPECIES']\n",
    "seed = 9\n",
    "validation_size = 0.3\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "print(X_train.shape, len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model and Evaluate\n",
    "dt_model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=seed) # Default Gini\n",
    "dt = dt_model.fit(X_train, Y_train)\n",
    "dt_prediction = dt.predict(X_test)\n",
    "print('Akurasi = ', accuracy_score(Y_test, dt_prediction))\n",
    "print(confusion_matrix(Y_test, dt_prediction))\n",
    "print(classification_report(Y_test, dt_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May not work in Google Colab, use Anaconda\n",
    "dot_data = tree.export_graphviz(dt, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\") \n",
    "var_names = ['SEPALLEN','SEPALWID', 'PETALLEN','PETALWID']\n",
    "categories = ['Setosa', 'VersiColor', 'Virginica']\n",
    "dot_data = tree.export_graphviz(dt, out_file=None, \n",
    "                         feature_names = var_names,  \n",
    "                         class_names=categories,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph \n",
    "# Interpretation is much easier from the graph\n",
    "# Installing GraphViz : https://stackoverflow.com/questions/49471867/installing-graphviz-for-use-with-python-3-on-windows-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varible importance\n",
    "dt.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively\n",
    "var = list(X_train.columns)\n",
    "viz = dtreeviz(dt,\n",
    "              X_train,\n",
    "              Y_train,\n",
    "              target_name='SPECIES',\n",
    "              feature_names=var,\n",
    "              class_names=['Setosa', 'VersiColor', 'Virginica']\n",
    "              )  \n",
    "display(HTML(viz.svg()))\n",
    "#viz.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/5_RandomForest.png\" style=\"width: 592px; height: 444px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curse of Dimensionality\n",
    "\n",
    "<p><img alt=\"\" src=\"images/chd_1.PNG\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mari coba perbaiki dengan Random Forest\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, Y_train)\n",
    "rf_prediction = rf.predict(X_test)\n",
    "print('Akurasi = ', accuracy_score(Y_test, rf_prediction))\n",
    "print(confusion_matrix(Y_test, rf_prediction))\n",
    "print(classification_report(Y_test, rf_prediction))\n",
    "# Ensemble less effective on strong classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varible importance\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "<img alt=\"\" src=\"images/naive_bayes.png\" style=\"width: 400px ; height: 220px\" />\n",
    "\n",
    "* P(x) konstan, sehingga bisa diabaikan.\n",
    "* Asumsi terkuatnya adalah independensi antar variabel prediktor (sehingga dikatakan &quot;Naive&quot;)\n",
    "* Klasifikasi dilakukan dengan menghitung probabilitas untuk setiap kategori ketika diberikan data x = (x1,x2,...,xm)\n",
    "* Untuk data yang besar bisa menggunakan out-of-core approach (partial fit):<br />\n",
    "\thttp://scikit-learn.org/stable/modules/scaling_strategies.html#scaling-strategies\n",
    "* Variasi NBC adalah bagaimana P(c|x) dihitung, misal dengan distribusi Gaussian (Normal) - sering disebut sebagai Gaussian Naive Bayes (GNB):\n",
    "\n",
    "<img alt=\"\" src=\"images/Gaussian.png\" style=\"width: 303px ; height: 50px\" />\n",
    "\n",
    "* very good explanation of NBC: \n",
    "* https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/ \n",
    "* https://www.saedsayad.com/naive_bayesian.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes in Social Media Analytics\n",
    "\n",
    "* Sentiment Analysis\n",
    "* Topic Modelling\n",
    "\n",
    "<p><img alt=\"\" src=\"images/SNA_Graph_Types.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><em><strong>Pros:</strong></em></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Cepat dan mudah di implementasikan</li>\n",
    "\t<li>Cocok untuk permasalahan multiclass</li>\n",
    "\t<li>Jika asumsi terpenuhi (independent) biasanya performanya cukup baik dan membutuhkan data (training) yang lebih sedikit.</li>\n",
    "\t<li>Biasanya baik digunakan untuk prediktor kategorik, untuk numerik NBC mengasumsikan distribusi normal (terkadang tidak terpenuhi)&nbsp;</li>\n",
    "</ul>\n",
    "\n",
    "<p><em><strong>Cons:</strong></em></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Jika di test data memuat kategori yang tidak ada di training data ( ==&gt; probabilitas = 0). Sering disebut sebagai masalah&nbsp; &ldquo;Zero Frequency&rdquo;.&nbsp;</li>\n",
    "\t<li>Asumsi yang sangat kuat (independen antar prediktor).</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "nbc = gnb.fit(X_train, Y_train)\n",
    "nbc_prediction = nbc.predict(X_test)\n",
    "\n",
    "print('Akurasi = ', accuracy_score(Y_test, nbc_prediction))\n",
    "print(confusion_matrix(Y_test, nbc_prediction))\n",
    "print(classification_report(Y_test, nbc_prediction))\n",
    "# Hati-hati Sparse ==> Dense bisa memenuhi memory untuk data relatif cukup besar\n",
    "# Akurasi cukup baik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Metrics in Python</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/6_Evaluasi_ML.JPG\" style=\"height:400px; width:515px\" /></p>\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "gnb = GaussianNB()\n",
    "mulai = time.time()\n",
    "scores_svm = cross_val_score(gnb, X, Y, cv=10,scoring='accuracy') # perhatikan sekarang kita menggunakan seluruh data\n",
    "waktu = time.time() - mulai\n",
    "# Interval Akurasi 95 CI \n",
    "print(\"Accuracy Naive Bayes Classifier: %0.2f (+/- %0.2f), Waktu = %0.3f detik\" % (scores_svm.mean(), scores_svm.std() * 2, waktu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparisons using Cross Validation\n",
    "Models = [('Naive Bayes',gnb), ('Random Forest',rf), ('Decision Tree',dt_model)]\n",
    "Scores = {}\n",
    "for model_name, model in Models:\n",
    "    Scores[model_name] = cross_val_score(model, X, Y, cv=10,scoring='accuracy')\n",
    "\n",
    "dt = pd.DataFrame.from_dict(Scores)\n",
    "ax = sns.boxplot(data=dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outline:\n",
    "\n",
    "* Support Vector Machines\n",
    "* Neural Network\n",
    "* Ensemble Models\n",
    "* Imbalance Data Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Modules untuk Notebook ini\n",
    "#!pip install imblearn\n",
    "#from imblearn.datasets import make_imbalance\n",
    "from imblearn.under_sampling import NearMiss # underSampling\n",
    "from imblearn.over_sampling import SMOTE # OverSampling\n",
    "from imblearn.combine import SMOTEENN # Combination of the 2\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "import numpy as np, matplotlib.pyplot as plt, pandas as pd, seaborn as sns\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import svm, preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import model_selection\n",
    "from collections import Counter\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbour\n",
    "<img alt=\"\" src=\"images/6_kNN.JPG\" style=\"height:300px; width:711px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "<img alt=\"\" src=\"images/reg_to_log.png\" style=\"width: 650px; height: 328px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teori Decision Tree : Information theory\n",
    "<img alt=\"\" src=\"images/dec_Tree_Theory.png\" style=\"width: 600px; height: 337px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "<img alt=\"\" src=\"images/naive_bayes.png\" style=\"width: 400px ; height: 220px\" />\n",
    "<ul>\n",
    "\t<li>P(x) konstan, sehingga bisa diabaikan.</li>\n",
    "\t<li>Asumsi terkuatnya adalah independensi antar variabel prediktor (sehingga dikatakan &quot;Naive&quot;)</li>\n",
    "\t<li>Klasifikasi dilakukan dengan menghitung probabilitas untuk setiap kategori ketika diberikan data x = (x1,x2,...,xm)</li>\n",
    "\t<li>Untuk data yang besar bisa menggunakan out-of-core approach (partial fit):<br />\n",
    "\thttp://scikit-learn.org/stable/modules/scaling_strategies.html#scaling-strategies</li>\n",
    "\t<li>Variasi NBC adalah bagaimana P(c|x) dihitung, misal dengan distribusi Gaussian (Normal) - sering disebut sebagai Gaussian Naive Bayes (GNB):</li>\n",
    "</ul>\n",
    "<img alt=\"\" src=\"images/Gaussian.png\" style=\"width: 303px ; height: 50px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/OverFitting.png\" style=\"width: 400px; height: 427px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/sweet_spot.png\" style=\"width: 400px; height: 236px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "Misal data dinyatakan sebagai berikut:\n",
    "$\\{(\\bar{x}_1,y_1),...,(\\bar{x}_n,y_n)\\}$, dimana $\\bar{x}_i$ adalah\n",
    "input pattern untuk data ke $i^{th}$ dan $y_i$ adalah nilai target yang diinginkan. Kategori\n",
    "(class) direpresentasikan dengan $y_i=\\{-1,1\\}$. Sebuah bidang datar (hyperplane) yang memisahkan kedua kelas ini (\"linearly separable\") adalah:\n",
    "$$ \\bar{w}'\\bar{x}+b=0 $$\n",
    "dimana $\\bar{x}$ adalah input vector (prediktor), $\\bar{w}$ weight, dan $b$ disebut sebagai bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pemodelan SVM (Hard Margin):\n",
    "\n",
    "<img alt=\"\" src=\"images/Pemodelan_SVM_.png\" style=\"width: 250px ; height: 269px\" />\n",
    "* Misal **Xo** adalah sebuah vector di bidang (plane/garis) _wX + b = -1_\n",
    "* Misal **r** adalah jarak antar SV-nya.\n",
    "* karena **X** berada di bidang _wX+b=1_ maka  _X=Xo+rw/||w||_ \n",
    "* (lihat gambar *w* tegak lurus *X* (karena _wX+b=0_) dan _w/||w||_ adalah unit vektornya)\n",
    "* Sehingga _wX+b=1_ dapat dituliskan sebagai _w(Xo+r w/||w||)-b = 1_\n",
    "* atau _wXo+r||w||²/||w||-b=1_ ==> _wXo-b=1-r||w||_ ==> _-1=1-r||w||_\n",
    "* sehingga di dapat $r = \\frac{2}{||w||}$\n",
    "* Kesimpulannya optimal hyperplane bisa didapatkan dengan memaksimumkan $\\frac{2}{||w||}$ atau setara dengan $\\min \\frac{||w||}{2}$\n",
    "* More details here: https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/hard_margin_svm.png\" style=\"width: 400px; height: 181px;\" />\n",
    "* Efek outlier pada pemodelan ini?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine: Soft Margin\n",
    "\n",
    "<img alt=\"\" src=\"images/6_SVM.jpg\" style=\"height: 262px ; width: 232px\" />\n",
    "<img alt=\"\" src=\"images/svm_opt.png\" style=\"width: 300px; height: 106px;\" />\n",
    "* Apakah efek outlier masih sama pada pemodelan ini? Kaitannya dengan nilai C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C >>> ==> toleransi terhadap outlier <<<< dan sebaliknya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual dan Quadratic solver\n",
    "* optimasi di atas biasanya diselesaikan dengan mencari bentuk *Dual*-nya.\n",
    "* Solusi untuk parameter optimalnya kemudian ditemukan dengan mencari pendekatan nilai optimalnya lewat Quadratic Programming solver.\n",
    "* Perhatikan bahwa bentuk fungsi optimasinya konvex ==> memiliki minimum global.\n",
    "* Nilai optimal dari pemodelan di atas hanya bergantung pada data-data di margin (support vector) sehingga bisa lebih efisien (jika SV telah diketahui).\n",
    "* SV juga dapat digunakan untuk menganalisa \"Error Bound\" : http://www.svms.org/vc-dimension/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "* Recursive Feature Elimination (RFE) method : https://link.springer.com/content/pdf/10.1023/A:1012487302797.pdf \n",
    "* melihat bentuk kuadrat dari setiap komponen *w* (higher better).\n",
    "* hati-hati beberapa diskusi di internet menyatakan bahwa sign (+/-) menyatakan tingkat kepentingan terhadap setiap variabel, namun hal ini tidak selalu benar dan bisa dibuktikan cukup dengan counter example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagaimana dengan data kategorik?\n",
    "* Sama dengan regresi (logistik) ==> Dummy (indicator variable) variable.\n",
    "* Misal X1 = {a,b,c} ==> X1_a = [1,0,0], X1_b = [0,1,0], X1_c = [0,0,1]\n",
    "* https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh\n",
    "df = pd.DataFrame({'X1': ['a', 'b', 'a','c','a'],'X2': [1, 2, 3, 2, 1]})\n",
    "df = pd.get_dummies(df) # get_dummies(df, prefix=['dummy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisasi/Standarisasi Data\n",
    "* Sama seperti Regresi (logistik) prediktor/features di model SVM perlu untuk di standarisasi/normalisasi.\n",
    "* http://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range\n",
    "* Hati-hati standarisasi data dilakukan setelah outlier ditangani dengan baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler(with_mean=True, with_std=True)\n",
    "df['X2'] = scaler.fit_transform(df[['X2']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh plotting Optimal Hyperplane\n",
    "# http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#example-svm-plot-separating-hyperplane-py\n",
    "\n",
    "X, y = make_blobs(n_samples=20, centers=2, random_state=6) # we create 20 separable points\n",
    "clf = svm.SVC(kernel='linear', C=1000) # fit the model, don't regularize for illustration purposes\n",
    "clf.fit(X, y)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "ax = plt.gca();xlim = ax.get_xlim(); ylim = ax.get_ylim()\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30);yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,linestyles=['--', '-', '--'])# plot decision boundary and margins\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,linewidth=1, facecolors='none', edgecolors='k')# plot support vectors\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SVM Kernel (trick)</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/6_SVM_Kernel.jpg\" style=\"height:168px; width:306px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definisi Fungsi Kernel\n",
    "* Jika untuk semua $\\bar{x},\\bar{z} \\in X$, memenuhi <br> \n",
    "$$\\kappa (\\bar{x},\\bar{z})=<\\phi (\\bar{x}),\\phi (\\bar{z})>$$ maka $\\kappa$ disebut fungsi Kernel (fungsi $\\phi$ disebut *feature map*).\n",
    "* Perhatikan hasil pemetaan fungsi kernelnya adalah scalar (inner product).\n",
    "* Fungsi ini digunakan di SVM (dan model DM/ML lain yang bisa dinyatakan dalam inner product).\n",
    "* Perhatikan pemodelan SVM; kebanyakan dinyatakan dalam inner product (i.e. w.x).\n",
    "* See here for more details: https://nlp.stanford.edu/IR-book/html/htmledition/nonlinear-svms-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Contoh-:-Lagrangian-Wolfe-Dual-dari-Optimasi-diatas\">Contoh : Lagrangian Wolfe Dual dari Optimasi diatas</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/Lagrangian_Wolfe_Dual.png\" style=\"width: 290px; height: 120px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contoh 1\n",
    "* Misal $X\\subseteq \\Re^2$ dan $\\phi : \\bar{x}=(x_1,x_2)\\rightarrow \\phi (\\bar{x})=(x_1^2,\n",
    "x_2^2,\\sqrt{2}x_1x_2)\\in F=\\Re^3$.\n",
    "* maka <br>\n",
    "$<\\phi(\\bar{x}),\\phi(\\bar{z})>$<br>\n",
    "$=<(x_1^2,x_2^2,\\sqrt{2}x_1x_2),(z_1^2,z_2^2,\\sqrt{2}z_1z_2)>$<br>\n",
    "$=x_1^2z_1^2+x_2^2z_2^2+2x_1x_2z_1z_2$<br>\n",
    "$=(x_1z_1+x_2z_2)^2=<\\bar{x},\\bar{z}>^2$<br>\n",
    "* Sehingga $\\kappa(\\bar{x},\\bar{z})=<\\bar{x},\\bar{z}>^2$ adalah sebuah fungsi kernel dan $F$ adalah ruang feature-nya (feature space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contoh 2\n",
    "* Misal x = (x1, x2, x3); y = (y1, y2, y3). \n",
    "* dan fungsi pemetaan variabelnya f(x) = (x1², x1x2, x1x3, x2x1, x2², x2x3, x3x1, x3x2, x3²), \n",
    "* maka kernelnya adalah K(x, y ) = <f(x), f(y)> = <x, y>².\n",
    "* Contoh numerik misal x = (1, 2, 3) dan y = (4, 5, 6). maka:\n",
    "* f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9) <br> f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)\n",
    "* <f(x), f(y)> = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024\n",
    "* complicated!... Menggunakan fungsi kernel perhitungannya bisa disederhanakan:\n",
    "* K(x, y) = (4 + 10 + 18)² = 32² = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Well-Known-Kernel-Functions\">Well-Known Kernel Functions</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/Well-Known_Kernels.png\" style=\"width: 400px; height: 208px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SVM Binary to MultiClass</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/6_SVM_Ova.jpg\" style=\"height:314px; width:432px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Pros</b></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Akurasinya Baik</li>\n",
    "\t<li>Bekerja dengan baik untuk sampel data yang relatif kecil</li>\n",
    "\t<li>Hanya bergantung pada SV ==&gt; meningkatkan efisiensi</li>\n",
    "\t<li>Convex ==&gt; Minimum Global ==&gt; Pasti Konvergen</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>Cons</b></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Tidak efisien untuk data yang besar</li>\n",
    "\t<li>Akurasi terkadang rendah untuk multiklasifikasi (sulit mendapatkan hubungan antar kategori di modelnya)</li>\n",
    "\t<li>Tidak robust terhadap noise</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh Binary SVM (dengan dan tanpa kernel)\n",
    "# Loading Data\n",
    "df = pd.read_excel('data/iris.xls', sheet_name='Sheet1')# Load Data from File\n",
    "df = df.loc[0:99] # Use only First 100 data (becomes a Binary problem)\n",
    "df['SPECIES'] = df['SPECIES'].astype('category')\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data\n",
    "X = df[['SEPALLEN','SEPALWID','PETALLEN','PETALWID']]\n",
    "Y = df['SPECIES']\n",
    "X = preprocessing.StandardScaler().fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and evaluate the model\n",
    "dSVM = svm.SVC(C = 10**5, kernel = 'linear')\n",
    "dSVM.fit(X_train, Y_train)\n",
    "y_SVM = dSVM.predict(X_test)\n",
    "print('Akurasi = ', accuracy_score(Y_test, y_SVM))\n",
    "print(confusion_matrix(Y_test, y_SVM))\n",
    "print(classification_report(Y_test, y_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Support Vectors\n",
    "print('index dr SV-nya: ', dSVM.support_)\n",
    "print('Vector Datanya: \\n', dSVM.support_vectors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Weights for interpretations\n",
    "print('w = ',dSVM.coef_)\n",
    "print('b = ',dSVM.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan Kernel: http://scikit-learn.org/stable/modules/svm.html#svm-kernels\n",
    "for kernel in ('sigmoid', 'poly', 'rbf'):\n",
    "    dSVM = svm.SVC(kernel=kernel)\n",
    "    dSVM.fit(X_train, Y_train)\n",
    "    y_SVM = dSVM.predict(X_test)\n",
    "    print(accuracy_score(Y_test, y_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh Multiklasifikasi SVM (dengan dan tanpa kernel)\n",
    "df = pd.read_excel('data/iris.xls', sheet_name='Sheet1')# Load Data from File\n",
    "df['SPECIES'] = df['SPECIES'].astype('category')\n",
    "X = df[['SEPALLEN','SEPALWID','PETALLEN','PETALWID']]\n",
    "Y = df['SPECIES']\n",
    "X = preprocessing.StandardScaler().fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3)\n",
    "print(X_train.shape, X_test.shape)\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Versus All: http://www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf\n",
    "dSVM = svm.LinearSVC()\n",
    "dSVM.fit(X_train, Y_train)\n",
    "y_SVM = dSVM.predict(X_test)\n",
    "print('Akurasi = ', accuracy_score(Y_test, y_SVM))\n",
    "y_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ada 3 classifier (as expected)\n",
    "dSVM.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All At Once Method http://www.jmlr.org/papers/volume2/crammer01a/crammer01a.pdf\n",
    "dSVM = svm.SVC(decision_function_shape='ovo')\n",
    "dSVM.fit(X_train, Y_train)\n",
    "y_SVM = dSVM.predict(X_test)\n",
    "print('Akurasi = ', accuracy_score(Y_test, y_SVM))\n",
    "y_SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Artificial-Neural-Network---Jaringan-Syaraf-tiruan\">Artificial Neural Network - Jaringan Syaraf Tiruan</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/JST.jpg\" style=\"width: 600px; height: 362px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/6_JST.JPG\" style=\"height:400px; width:706px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/6_JST_calculation.JPG\" style=\"height:350px; width:638px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/6_JST_Actv.png\" style=\"height:400px; width:484px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Toy-Data-Example-Neural-Network-(Back-Propagation)\">Toy Data Example Neural Network (Back Propagation)</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/NN-BP_step_1.png\" style=\"width: 600px; height: 358px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/NN-BP_step_2.png\" style=\"width: 600px; height: 348px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/NN-BP_step_3.png\" style=\"width: 600px; height: 313px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/NN-BP_step_4.png\" style=\"width: 600px; height: 305px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/NN-BP_step_5.png\" style=\"width: 600px; height: 305px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/NN-BP_step_6.png\" style=\"width: 600px; height: 304px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/NN-BP_step_7.png\" style=\"width: 600px; height: 305px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/NN-BP_step_8.png\" style=\"width: 600px; height: 303px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/NN-BP_step_9.png\" style=\"width: 600px; height: 301px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass ANN\n",
    "<img alt=\"\" src=\"images/Multiclass_ANN.png\" style=\"width: 600px; height: 468px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melihat pemodelan Matematis dan cara kerja Neural Network, apakah kita perlu melakukan standarisasi data juga seperti SVM dan Regresi Logistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Neural Network - Empirical Analysis Parameter di ANN</p>\n",
    "<strong><a href=\"https://goo.gl/3rcnc9\" target=\"_blank\">https://goo.gl/3rcnc9</a></strong>\n",
    "\n",
    "\n",
    "<p>Mengapa dengan fungsi linear bisa membentuk &quot;boundary&quot; yang melengkung (kurva)?</p>\n",
    "<strong><a href=\"http://s.id/j6i\" target=\"_blank\">http://s.id/j6i</a></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/6_tipe_NN.png\" style=\"height:400px; width:711px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network VS Deep Learning\n",
    "<img alt=\"\" src=\"images/5_DeepLearning.png\" style=\"width: 600px; height: 676px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/6_NN_when_to_use.JPG\" style=\"height:400px; width:499px\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network: http://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "NN = MLPClassifier(hidden_layer_sizes=(100,))# 2 layers 30 neurons and 20 neurons\n",
    "NN.fit(X_train, Y_train)\n",
    "y_NN = NN.predict(X_test)\n",
    "print('Akurasi = ', accuracy_score(Y_test, y_NN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Induktif bias :\n",
    "<ul>\n",
    "\t<li>Bias penaksiran parameter (statistik)</li>\n",
    "\t<li>Induktif Bias Sample (Machine Learning - Tom Mitchel)</li>\n",
    "\t<li>Induktif Bias Pemilihan Classifier (Statistical Learning Theory - Vapnik)</li>\n",
    "</ul>\n",
    "<img alt=\"\" src=\"images/inductive_biases_.png\" style=\"width: 600px; height: 153px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h, i = .02, 1  # step size in the mesh , iterate over datasets\n",
    "names = [\"Nearest Neighbors\", \"Logistic Regression\", \"Naive Bayes\", \"Linear SVM\", \"RBF SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\"]\n",
    "\n",
    "classifiers = [KNeighborsClassifier(3),\n",
    "    LogisticRegression(solver='lbfgs',multi_class='multinomial'),\n",
    "    GaussianNB(), SVC(kernel=\"linear\", C=0.025), SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1)]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),make_circles(noise=0.2, factor=0.5, random_state=1),linearly_separable]\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = preprocessing.StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max()); ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(()); ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max());ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(()); ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout();plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Ensemble-Model\">Ensemble Model</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>What? a learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions.</li>\n",
    "\t<li>Why? Better prediction, More stable model</li>\n",
    "\t<li>How? Bagging &amp; Boosting</li>\n",
    "</ul>\n",
    "<img alt=\"\" src=\"images/Ensemble.png\" style=\"width: 500px; height: 213px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## “meta-algorithms” : Bagging & Boosting\n",
    "* Ensemble https://www.youtube.com/watch?v=Un9zObFjBH0 \n",
    "* Bagging https://www.youtube.com/watch?v=2Mg8QD0F1dQ \n",
    "* Boosting https://www.youtube.com/watch?v=GM3CDQfQ4sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/Bagging_VS_Boosting.png\" style=\"width: 500px; height: 185px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/Bagging-Boosting_Usage.png\" style=\"width: 500px; height: 281px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Ada-Boost\">AdaBoost</h2>\n",
    "<ul>\n",
    "\t<li><a href=\"https://youtu.be/BoGNyWW9-mE?t=70\" target=\"_blank\">https://youtu.be/BoGNyWW9-mE?t=70</a></li>\n",
    "</ul>\n",
    "<img alt=\"\" src=\"images/AdaBoost.png\" style=\"width: 400px; height: 332px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh Voting (Bagging) di Python\n",
    "# Catatan : Random Forest termasuk Bagging Ensemble (walau modified)\n",
    "file = 'data/diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = pd.read_csv(file, names=names).values # Rubah ke numpy array\n",
    "X, Y = data[:,0:8], data[:,8] # Slice\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3)\n",
    "\n",
    "kNN = KNeighborsClassifier(3)\n",
    "kNN.fit(X_train, Y_train)\n",
    "Y_kNN = kNN.score(X_test, Y_test)\n",
    "\n",
    "DT = DecisionTreeClassifier(random_state=1)\n",
    "DT.fit(X_train, Y_train)\n",
    "Y_DT = DT.score(X_test, Y_test)\n",
    "\n",
    "model = VotingClassifier(estimators=[('k-NN', kNN), ('Decision Tree', DT)], voting='hard')\n",
    "model.fit(X_train,Y_train)\n",
    "Y_Vot = model.score(X_test,Y_test)\n",
    "\n",
    "print('Akurasi k-NN', Y_kNN)\n",
    "print('Akurasi Decision Tree', Y_DT)\n",
    "print('Akurasi Votting', Y_Vot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging juga bisa digunakan di Klasifikasi (ndak hanya Regresi), \n",
    "# tapi kita pakai probabilitas dari setiap kategori\n",
    "T = DecisionTreeClassifier()\n",
    "K = KNeighborsClassifier()\n",
    "R= LogisticRegression()\n",
    "\n",
    "T.fit(X_train,Y_train)\n",
    "K.fit(X_train,Y_train)\n",
    "R.fit(X_train,Y_train)\n",
    "\n",
    "y_T=T.predict_proba(X_test)\n",
    "y_K=K.predict_proba(X_test)\n",
    "y_R=R.predict_proba(X_test)\n",
    "\n",
    "Ave = (y_T+y_K+y_R)/3\n",
    "print(Ave[:5]) # Print just first 5\n",
    "prediction = [v.index(max(v)) for v in Ave.tolist()]\n",
    "print(prediction[:5]) # Print just first 5\n",
    "print('Akurasi Averaging', accuracy_score(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "num_trees = 100\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=9)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=1)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Imbalance-Data\">Imbalance Data</h2>\n",
    "* Metric Trap\n",
    "* Akurasi kategori tertentu lebih penting\n",
    "* Contoh kasus\n",
    "<img alt=\"\" src=\"images/imbalance.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\t<li>Undersampling</li>\n",
    "\t<li>Oversampling</li>\n",
    "\t<li>Model Based (weight adjustment)</li>\n",
    "</ul>\n",
    "<img alt=\"\" src=\"images/under-over-sampling.png\" style=\"width: 500px; height: 147px;\" />\n",
    "* https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
    "* Plot perbandingan: https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/combine/plot_comparison_combine.html#sphx-glr-auto-examples-combine-plot-comparison-combine-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh undersampling\n",
    "# jalankan perintah ini di terminal/command prompt: \n",
    "# \"pip install imblearn\" (Jupyter harus ditutup terlebih dahulu)\n",
    "#from imblearn.datasets import make_imbalance\n",
    "from imblearn.under_sampling import NearMiss # underSampling\n",
    "from imblearn.over_sampling import SMOTE # OverSampling\n",
    "from imblearn.combine import SMOTEENN # Combination of the 2\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "file = 'data/diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = pd.read_csv(file, names=names).values # Rubah ke numpy array\n",
    "X, Y = data[:,0:8], data[:,8].astype(int) # Slice\n",
    "X = preprocessing.StandardScaler().fit_transform(X)\n",
    "X, Y = make_imbalance(X, Y, sampling_strategy={0: 500, 1: 50},random_state=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3)\n",
    "\n",
    "print('Training target statistics: {}'.format(Counter(Y_train)))\n",
    "print('Testing target statistics: {}'.format(Counter(Y_test)))\n",
    "\n",
    "dSVM = svm.LinearSVC()\n",
    "dSVM.fit(X_train, Y_train)\n",
    "y_SVM = dSVM.predict(X_test); del dSVM\n",
    "print('Original Results:',classification_report(Y_test, y_SVM))\n",
    "\n",
    "nm = NearMiss(random_state=1)\n",
    "X_res, Y_res = nm.fit_resample(X_train, Y_train)\n",
    "dSVM = svm.LinearSVC()\n",
    "dSVM.fit(X_res, Y_res)\n",
    "y_SVM = dSVM.predict(X_test); del dSVM\n",
    "print('UnderSampling Results:\\n',classification_report_imbalanced(Y_test, y_SVM))\n",
    "\n",
    "sm = SMOTE(random_state=1)\n",
    "X_res, Y_res = sm.fit_resample(X_train, Y_train)\n",
    "dSVM = svm.LinearSVC()\n",
    "dSVM.fit(X_res, Y_res)\n",
    "y_SVM = dSVM.predict(X_test); del dSVM\n",
    "print('OverSampling Results:\\n',classification_report_imbalanced(Y_test, y_SVM))\n",
    "\n",
    "\n",
    "smo = SMOTEENN(random_state=1)\n",
    "X_res, Y_res = smo.fit_resample(X_train, Y_train)\n",
    "dSVM = svm.LinearSVC()\n",
    "dSVM.fit(X_res, Y_res)\n",
    "y_SVM = dSVM.predict(X_test)\n",
    "print('Combination Results:\\n',classification_report_imbalanced(Y_test, y_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of model-based imbalance treatment - SVM\n",
    "n_samples_1, n_samples_2 = 1000, 100\n",
    "centers = [[0.0, 0.0], [2.0, 2.0]]\n",
    "clusters_std = [1.5, 0.5]\n",
    "X, y = make_blobs(n_samples=[n_samples_1, n_samples_2],centers=centers,cluster_std=clusters_std,random_state=0, shuffle=False)\n",
    "\n",
    "# fit the model and get the separating hyperplane\n",
    "clf = svm.SVC(kernel='linear', C=1.0)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# fit the model and get the separating hyperplane using weighted classes\n",
    "wclf = svm.SVC(kernel='linear', class_weight={1: 10}) #WEIGHTED SVM\n",
    "wclf.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')# plot the samples\n",
    "ax = plt.gca()# plot the decision functions for both classifiers\n",
    "xlim = ax.get_xlim(); ylim = ax.get_ylim()\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)# create grid to evaluate model\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)# get the separating hyperplane\n",
    "a = ax.contour(XX, YY, Z, colors='k', levels=[0], alpha=0.5, linestyles=['-']) # plot decision boundary and margins\n",
    "Z = wclf.decision_function(xy).reshape(XX.shape)# get the separating hyperplane for weighted classes\n",
    "b = ax.contour(XX, YY, Z, colors='r', levels=[0], alpha=0.5, linestyles=['-'])# plot decision boundary and margins for weighted classes\n",
    "plt.legend([a.collections[0], b.collections[0]], [\"non weighted\", \"weighted\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = pd.read_csv(file, names=names).values # Rubah ke numpy array\n",
    "X, Y = data[:,0:8], data[:,8].astype(int) # Slice\n",
    "X = preprocessing.StandardScaler().fit_transform(X)\n",
    "X, Y = make_imbalance(X, Y, sampling_strategy={0: 500, 1: 50},random_state=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3)\n",
    "\n",
    "del T\n",
    "T = DecisionTreeClassifier(random_state = 0)\n",
    "T.fit(X_train,Y_train)\n",
    "y_DT = T.predict(X_test)\n",
    "print('Akurasi  (Decision tree Biasa) = ', accuracy_score(Y_test, y_DT))\n",
    "print(classification_report(Y_test, y_DT))\n",
    "\n",
    "del T\n",
    "T = DecisionTreeClassifier(class_weight = 'balanced', random_state = 0)\n",
    "T.fit(X_train,Y_train)\n",
    "y_DT = T.predict(X_test)\n",
    "print('Akurasi  (Weighted Decision tree) = ', accuracy_score(Y_test, y_DT))\n",
    "print(classification_report(Y_test, y_DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> End of Module IV\n",
    "\n",
    "<hr />\n",
    "<p><img alt=\"\" src=\"images/meme_09_Predictive_model_insights.jpg\" /></p>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
